# 信息与熵

### 信息与熵

生活中，我们常常会提到“这句话信息量很大”，这是指某句话中包含了很多很多的信息。那么信息如何度量其大小呢？什么样的信息可以说信息量大呢？信息论里把信息量的大小和信息的不确定性联系起来，即，一条信息的信息量大小和它的不确定性有关。比如，某件事情我们一无所知，那么为了弄清楚它，我们就需要了解大量的信息；而如果一件事情非常确定，或者了解的非常清楚，那么我们就不需要太多的信息。实际上，信息量的大小就是对不确定性的一种度量。

为了能够用数学语言描述信息量的大小，我们希望我们的数学工具能够满足以下几个性质：

- 发生概率大的事件，需要的信息量要小；在极端情况下，如果某事件确定能够发生，那么该事件对应的信息量为0；
- 发生概率小的事件对应的信息量要大；
- 相互独立的事件的信息量应该要满足可加性，即，如果我们有两个不相关的事件x和y，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：I(x,y)=I(x)+I(y)。

第三条性质其实给我们建立数学工具很大的启发。我们可以看到，两个相互独立的事件发生，所需要的信息量的大小是两个事件独立发生信息量之和，这个性质其实与对数有着非常好的对应关系。在对数中，我们有一条简单的性质：

$log(xy) = log(x) + log(y)$

为了满足以上三条性质，我们定义自信息：

$I(x) = − logP(x)$

这里由于$P(x)$不会超过1，因此需要前面有个负号来保证信息量为正。在信息论中，对数的基常常选择为2，因此信息的单位为比特bits；而机器学习中，对数的基常常选择为自然常数，因此单位常常被称为奈特nats。

自信息表述的是某个特定的值所具有的信息，当然，我们可以对整个概率分布进行表述，这样得到的就是信息熵（香农熵）：

$H(x) = E_{x∼P}[I(x)] = E_{x∼P}[− logP(x)] = - \Sigma P(x)logP(x) = - \Sigma_{i=1}^{n}P(x_i)logP(x_i)$

从公式中我们不难发现：如果随机变量的数量越多，状态数越多；则熵越大；当随机变量服从均匀分布时，熵最大。

### 条件熵

条件熵指的是在随机变量X已知的情况下，随机变量Y的不确定性。

$H(Y|X) = \Sigma_{x}P(x)H(Y|X=x)=-\Sigma_{x}P(x)\Sigma_{y}P(y|x)logP(y|x)=-\Sigma_x\Sigma_yP(x,y)logP(y|x)$

条件熵实际上等于联合熵减去单独的熵。

$$
H(X,Y)=-\Sigma{x,y}P(x,y)logP(x,y)=-\Sigma{x,y}P(x,y)log(P(y|x)P(x)) \

=-\Sigma{x,y}P(x,y)logP(y|x)-\Sigma{x,y}P(x,y)logP(x)=H(Y|X)-\Sigma{x}logP(x)\Sigma{y}P(x,y)=H(Y|X)-\Sigma_{x}P(x)logP(x)=H(Y|X)+H(X)
$$

### KL散度

$P(x)$，$Q(x)$是离散随机变量X中取值的两个概率分布：

$D_{KL}(P||Q)=E_{x∼P}[log\dfrac{P(x)}{Q(x)}]=E_{x∼P}[logP(x)-logQ(x)]=\Sigma_x P(x)logP(x)-P(x)logQ(x)=-H(P)-\Sigma_xP(x)logQ(x)$

KL散度可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 P 与 Q 之间的对数差在 P 上的期望值。

### 交叉熵

交叉熵就是KL散度加上P的熵。这一点从KL散度的公式推导过程中就能看出来。交叉熵的意义其实是这样，如果实际的分布是P(x)，但是我们希望用Q(x)这个分布去拟合实际的分布P(x)，需要的编码长度就是交叉熵。

$H(P,Q)=-\Sigma_xP(x)logQ(x)$

### 最小化

机器学习中常常会遇到需要最小化交叉熵。对于我们的机器学习问题，实际上是要让数据和模型的分布的KL散度尽可能小。由于数据的分布是固定的（$H(P)$），因此，最小化KL散度和最小化交叉熵是等价的。另外，最小化交叉熵还等价于最大后验。这一点可能不太容易理解。

首先我们改写一下KL散度

$D_{KL}(p_{data}||p_{model})=E_{x∼p_{data}}[log p_{data}-log p_{model}]$

KL散度等号右边的第一项其实与训练过程无关，数据是某个分布直接生成的，数据的分布也不会随着训练过程而发生变化。因此，最小化KL散度实际上就是最小化$-log p_{model}$，也就是最大化$log p_{model}$。再进一步把公式细化一些，最大化$log p_{model}$，实际上就是根据数据调整参数的值，也就是最大化$log p_{model}(\theta|x)$。这就是最大化似然函数。。。