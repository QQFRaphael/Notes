# 朴素贝叶斯

# 贝叶斯决策

假设我们有N个可能的分类，记为$Y={c_1,c_2,...,c_N}$。$\lambda_{ij}$是把一个实际上是$c_j$的样本错误分类为$c_i$造成的损失，那么，基于后验概率$P(c_i|x)$可以计算出把样本$x$分类为$c_i$后所产生的期望损失：

$$
R(c_i|x)=\sum_{j=1}^{N}\lambda_{ij}P(c_j|x)
$$

我们实际上需要寻找的是一个判别准则$h: X->Y$，能够让整体的损失尽可能的小:

$$
R(h)=E_x[R(h(x)|x)]
$$

从上面这个公式可以看出，如果想最小化$R(h)$，那么其实只需要最小化$R(h(x)|x)$。这样就导出了贝叶斯判定准则：要最小化整体风险$R(h)$，只需要在每个样本上选择那个能使条件风险$R(c|x)$最小的类别标记：

$$
h^*(x)={argmin}_{c\in Y}R(c|x)
$$

这个$h^*(x)$被称为贝叶斯最优分类器。对应的整体风险$R(h^*(x))$被称为贝叶斯风险。

# 朴素贝叶斯

如果我们假设特征之间是相互独立的，那么我们就可以改写贝叶斯公式：

$$
P(c|x)=\dfrac{P(c)P(x|c)}{P(x)}=\dfrac{P(c)}{P(x)}\prod_{i=1}^d P(x_i|c)
$$

其中，$d$是特征的数目，$x_i$是$x$在第$i$个特征上的取值。

$P(x)$是整体数据表现出来的特征，因而对于所有类别来说是一样的。这样我们就可以得到贝叶斯判定准则：

$$
h(x)=argmaxP(c)\prod_{i=1}^d P(x_i|c)
$$

如果令$D_c$表示训练集$D$中第$c$类样本组成的集合，如果独立同分布的样本足够多，那么就很容易估算出先验概率：

$$
P(c)=\dfrac{|D_c|}{|D|}
$$

对于离散的特征，可以用$D_{c,x_i}$表示第$i$个特征上取值为$x_i$的样本组成的集合，那么条件概率$P(x_i|c)$可以估计为：

$$
P(x_i|c)=\dfrac{|D_{c,x_i}|}{|D_c|}
$$

对于连续的特征，可以假定该特征的$P(x_i|c)$服从高斯分布。

朴素贝叶斯把样本分类到后验概率最大的类中，这等价于期望风险最小化。

# 拉普拉斯平滑

朴素贝叶斯在一些情况下会遇到问题。比方说，训练完成后，实际业务中出现了某个不包含在训练集中的样本。那么这时候朴素贝叶斯方法在计算时就会出现$0/0$的情况，导致分类失效。为了避免这种情况出现，引入了拉普拉斯平滑的方法。

拉普拉斯平滑实际上就是修正先验概率和似然的计算：

$$
P(c)=\dfrac{|D_c|+1}{|D|+N} \\
P(x_i|c)=\dfrac{|D_{c,x_i}|+1}{|D_c|+K_i}
$$

公式中的$N$是训练集可能的类别数，$K_i$是第$i$个特征可能的取值数。